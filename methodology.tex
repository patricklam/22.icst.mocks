\label{sec:methodology}

We have implemented a tool \textsc{MockDetector} to perform fine-grained analysis, detecting invocations of mock objects in Java bytecode. \textsc{MockDetector} has a declarative implementation built on top of Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, which is easy to extend. For instance, marking a field as mock-containing in Doop was quite easy (we added 3 rules).
%We started this project with the usual imperative approach to implementing a static analysis---in our context, that meant using Soot. Then, when we wanted to experiment with adding more features to the analysis, we decided that this was a good opportunity to learn about Doop's declarative approach as well. We added new features to the Doop implementation and backported them to the Soot implementation. 
%Although the reason that two implementations exist is unimportant at this stage, the result is consequential:
%The existence of these two implementations is a rare opportunity to cross-check static analysis results. We thus carefully compared our results to make sure that they matched.
%Since the focus of this paper is on mock analysis, we will not discuss specific implementation details in depth.
%While the core analysis is similar, the different implementation technologies have different affordances. For instance, it is easier for the Doop version to mark a field as mock-containing (we added 3 rules) than for the Soot version to do so. We start by describing each implementation in turn, and conclude this section with the commonalities between the two implementations. Section~\ref{sec:evaluation} then presents the results obtained using each technology and compares them.

%\subsection{Declarative Doop Implementation}
%\label{sec:dec-doop}
%We next describe the declarative Doop-based technique that \textsc{MockDetector} uses. Both implementations 
As we saw in Section~\ref{sec:motivating-example}, the main idea is to propagate mockness from known mock sources, through the statements in the intermediate representation (IR), to potential mock invocation sites. Doop analyses specify propagation using Datalog rules. 
%Adding (context-insensitive) interprocedural support to a Doop analysis is almost trivial: we only needed to add two rules.
Our analysis is context-insensitive interprocedural.
It also supports arrays, containers, and fields. Although declarative analyses are easier to define than their imperative counterparts, they are still subject to the feature interaction problem: for instance, they still must specifically handle the case where an array-typed field is assigned from a mock-containing array local.

%We chose to implement a may-analysis rather than a must-analysis for two reasons: 1) we did not observe any cases where a value was assigned a mock on one branch and a real object on the other branch; 2) implementing a must-analysis would not help heuristics to find focal methods, as a must-analysis would rule out fewer mock invocations. 


% Mock Libraries discussed in Common Infrastructure section, perhaps refer to the paragraph in Common Infrastructure section?
The implementation declares facts for 10 mock source methods manually gleaned from the mock libraries' documentation, as specified through method signatures (e.g. 
\texttt{<org.mockito.Mockito: java.lang.Object mock(java.lang.Class)>}.)
It then declares that a variable {\tt v} satisfies \verb+isMockVar(v)+ if it is assigned from the return value of a mock source, or otherwise traverses the program's interprocedural control-flow graph, through assignments, which may possibly flow through fields, collections, or arrays. Finally, an invocation site is a mock invocation if the receiver object {\tt v} satisfies \verb+isMockVar(v)+.

\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none,label={lst:core},caption={Selected rules for Datalog mock analysis},
basicstyle=\scriptsize\ttfamily, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, label=lis:datalog-rule, escapechar=!]
// v = mock()
isMockVar(v) :-
  AssignReturnValue(mi, v),
  callsMockSource(mi).
// v = (type) from
isMockVar(v) :- isMockVar(from),
  AssignCast(_/* type */, from, v, _/* inmethod */).
// v = v1
isMockVar(v) :- isMockVar(v1),
  AssignLocal(v1, v, _).
\end{lstlisting}

The Doop-provided predicates \texttt{AssignReturnValue}, \texttt{AssignCast}, and \texttt{AssignLocal} resemble Java bytecode instructions; for instance, \texttt{AssignLocal(?from:Var, ?to:Var, ?inmethod:Method)} is an assignment statement copying from variable \texttt{?from} to \texttt{?to} in method \texttt{?inmethod}. (It is Datalog convention to prefix parameters with \texttt{?}s).

We designed the analysis in a modular fashion, such that the interprocedural, collections, arrays, and fields support can all be disabled through the use of \verb+#ifdef+s, which can be specified on the Doop command-line.

\paragraph{Interprocedural support}
As mentioned above, it was quite easy to implement interprocedural analysis in our Doop implementation; we leveraged Doop's call graph and simply propagated mockness information along its call graph edges.

\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none,caption={Two rules give interprocedural analysis in Doop.},
basicstyle=\scriptsize\ttfamily, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, label=lis:interproc-rule, escapechar=!]
// v = callee(), where callee's return var is mock
isInterprocMockVar(v) :-
  AssignReturnValue(mi, v),
  mainAnalysis.CallGraphEdge(_, mi, _, callee),
  ReturnVar(v_callee, callee),
  isMockVar(v_callee).

// callee(v) results in formal
//   param of callee being mock
isInterprocMockVar(v_callee) :- isMockVar(v),
  ActualParam(n, mi, v),
  FormalParam(n, callee, v_callee),
  mainAnalysis.CallGraphEdge(_, mi, _, callee),
  Method_DeclaringType(callee, callee_class),
  ApplicationClass(callee_class).
\end{lstlisting}
We use Doop's call graph edges (\texttt{mainAnalysis.CallGraphEdge}) between the method invocation {\tt mi} and its callee {\tt callee}; the first rule propagates information from callees back to their callers, while the second rule propagates information from callers to callees through parameters. We restrict our analysis to so-called ``application classes'', excluding in particular the Java standard library. We chose to run our context-insensitive analysis on top of Doop's context-insensitive call graph. Mirroring Doop itself, it would also be possible to add context sensitivity to our analysis, potentially reducing the number of false positives.%, but our results suggest that this would not help much; we'll return to that point in Section~\ref{sec:evaluation}.
\paragraph{Arrays and Containers} We record local variables pointing to mock-containing arrays using  {\tt isArrayLocalThatContainsMocks}. This predicate is true whenever the program under analysis stores a mock variable into an array; we also transfer array-mockness through assignments and casts. When a local variable \texttt{v} is read from a mock-containing array \texttt{c}, then \texttt{v} is marked as a mock variable, as seen in the first rule below. A store of a mock variable \texttt{mv} into an array \texttt{c} causes that array to be \texttt{isArrayLocalThatContainsMocks}. Note that these predicates are mutually-recursive. 

\begin{lstlisting}[basicstyle=\ttfamily\small,numbers=none,caption={Rules for handling arrays.},
basicstyle=\scriptsize\ttfamily, framesep=4.5mm, framexleftmargin=1.0mm, captionpos=b, label=lis:array-rule, escapechar=!]
// v = c[idx]
isMockVar(v) :-
  isArrayLocalThatContainsMocks(c),
  LoadArrayIndex(c, v, _ /* idx */).

// c[idx] = mv
isArrayLocalThatContainsMocks(c) :-
  StoreArrayIndex(mv, c, _ /* idx */),
  isMockVar(mv).
\end{lstlisting}

We treat collections analogously. However, while there is one API for arrays---bytecode array load and store instructions---Java's Collections APIs include, by our count, 60 relevant methods. We support iterators, collection copies via constructors, and add-all methods. We use our classification of collection methods to identify collection reads and writes and handle them as we do array reads and writes. We also handle {\tt Collection.toArray} by propagating mockness from the collection to the array.%, except that we say that it is a mock-containing collection, not a mock-containing array.

\paragraph{Fields} Apart from the obvious rule stating that a field which is assigned from a mock satisfies {\tt fieldContainsMock}, we also label fields that have the {\tt org.mockito.Mock} or \\{\tt org.easyMock.Mock} annotations as mock-containing. We declare that a given field \emph{signature} may contain a mock, i.e. the field with a given signature belonging to all objects of a given type. We also support containers stored in fields.

\paragraph{Arrays and Fields} We also support not just array locals but also array fields. That is, when an array-typed field is assigned from a mock-containing array local, then it is also a mock. And when an array-typed local variable is assigned from a mock-containing array field, then that array local is a mock-containing array.



%Section~\ref{sec:methodology} will provide a more detailed definition of ``hard to stub''. We have identified a number of reasons that make a class or interface hard to stub. Some reasons are: would have to implement too many methods (even if the implementation is empty); has no accessible constructor; constructor has too many arguments; etc. Our goal is to assess how often mocks are used to provide implementations of classes or interfaces that are hard to stub. In other words, the use of a mock is indispensible for testing collaborators of these objects.

\paragraph{JUnit and Driver Generation}
Static analyses rely on having a set of program entry points. This is a bit tricky for JUnit tests.
Such tests are simply methods that developers write in test classes, appropriately annotated (in JUnit 3 by method name starting with ``test'', in 4+ by a \texttt{@Test} annotation). A JUnit test runner uses reflection to find tests. Thus, out of the box, static analysis engines do not see tests as reachable code.

% what about hierarchical drivers?

To enable static analysis over a benchmark's test suite, our tool uses the Soot program analysis framework~\cite{Vallee-Rai:1999:SJB:781995.782008} to generate a driver class for each Java sub-package of the suite (e.g. \texttt{org.apache.ibatis.executor.statement}). In each of these sub-package driver classes, our tool creates a \textit{runall()} method, which invokes all methods within the sub-package that JUnit considers tests, as well as non-constructor test cases, all surrounded by calls to class-level init/setup and teardown methods. Concrete test methods are particularly easy to call from a driver, as they are specified to have no parameters and are not supposed to rely on any particular ordering. 
Our tool then creates a RootDriver class at the root package level, which invokes the \textit{runall()} method in each sub-package driver class, along with the \texttt{@Test}/\texttt{@Before}/\texttt{@After} methods found in classes located at the root. The drivers that we generate also contain code to catch all checked exceptions declared to be thrown by the unit tests. Both our Soot and Doop implementations use the generated driver classes.

All static frameworks must somehow approximate the set of entry points as appropriate for their targets. For instance, the Wala framework~\cite{wala19:_t} also creates synthetic entry points, but it does this to perform pointer analysis on a program's main code rather than to enumerate the program's test cases.
