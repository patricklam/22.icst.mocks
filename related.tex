\label{sec:related}
We survey related work on mock usage and on empirical uses of language features.

\paragraph{Mock usage} A number of researchers have investigated mock usage. The novelty of our approach is that we collect finer-grained data on mocks. This data allows us to answer questions that were out of reach of previous approaches, and is also useful for subsequent analyses of test cases.

Mostafa and Wang carried out the first empirical study on mock usage~\cite{mostafa14:_empir_study_usage_mockin_framew_softw_testin}. Their description of mocking favours a classical view rather than a mockist view---their motivating example mocks an email server, which is a good example of an indispensable mock, as it is difficult to use a real email server during testing. They gathered data on how often mock objects were used over a sample of 5,000 open-source projects from GitHub, and found that 23\% of their projects with tests used mocks; of these projects with mocks, 30\% of the test classes used mocks. In terms of distance, they found that 39\% of mocks were from libraries, while the remaining 61\% were of classes in the system under test, which leans towards a mockist view (``only mock classes you own''). Our work goes beyond Mostafa and Wang in investigating more fine-grained properties of the classes that are mocked, in an attempt to determine how easy they are to stub.

More recently, Spadini et al~\cite{spadini19:_mock_java} performed a more in-depth examination of mock usage. Their focus was to understand \emph{why} developers wrote mocks, as well as gathering data on mock evolution over time and on coupling between the system under test and the mocks. They do not take a position on the classical versus mockist view; they attempt to determine which view is more compatible with extant software. They focus their attention on 4 software systems and interviewed developers to understand their motivations. Our work complements theirs; we collect additional data on which classes are hard to mock. Our automatically-collected data is more fine-grained (we have per-method-call information about whether a call is to mock or not, while they only track whether classes are mocks). They also categorize mocks into categories (e.g. databases, web services, etc), which is beyond the scope of our work. Their category-based mocking results thus capture more general concepts, while our results are more tied to the specific properties of different implementations. In the end, their qualitative results, including interviews with developers of software systems as well as a developer of the Mockito testing framework, show support for the classical view rather than the mockist view.

\paragraph{Language feature usage}
We also mention a body of work investigating language feature usage. Mocks are provided by libraries and are not a core language feature, though they can be considered a domain-specific language~\cite{freeman06:_evolv_embed_domain_specif_languag_java}. Okur and Dig~\cite{Okur:2012:DUP:2393596.2393660} also investigate library usage for parallel libraries. With respect to language features, Richards et al~\cite{Richards:2011:EML:2032497.2032503} surveyed usage of \texttt{eval} in deployed JavaScript code, classifying uses, while Morandat et al~\cite{Morandat:2012:EDR:2367163.2367172} investigate the use of various language features in R.

One language feature that attracted much recent attention was Rust unsafe, and three groups published overlapping empirical studies of its usage. Qin et al \cite{qin20:_under_memor_thread_safet_pract} manually investigate usages of unsafe Rust code, while Astrauskas et al~\cite{DBLP:journals/pacmpl/AstrauskasMP0S20} and Evans et al~\cite{DBLP:conf/icse/EvansCS20} carry out automated analyses across significant collections of Rust code. Like much work in this area, this line of work generally categorizes uses of unsafe code, and aims to determine how much of it might actually lead to property violations.

%% We discuss related work in the areas of focal method detection,
%% % declarative versus imperative static analysis,
%% treatment of containers, and taint analysis.

%% \paragraph{Focal methods and classes} To situate Ghafari et al's work~\cite{ghafari15:_autom}, a number of previous works have studied test-to-code traceability by identifying focal \emph{classes} for a test case---the classes which are tested by a test case. The focal \emph{methods} we discuss in this paper belong to focal classes. Ghafari et al were the first to extend the study of traceability to focal methods. Before that, Qusef et al proposed techniques which identify focal classes. In~\cite{DBLP:conf/icsm/QusefBOLB11}, they propose a two-stage approach relying on the assumption that the last assertion statement in a test case is the key assertion. The first stage uses dynamic slicing to find all classes that contribute to the values tested in the assertion (possibly including mocks), while the second stage filters classes and keeps only those textually closest to the test class. An additional mock object filter would help remove definitely-not-focal classes. Earlier work by Qusef et al~\cite{DBLP:conf/icsm/QusefOL10} uses dataflow analysis instead of dynamic slicing.

%% Rompaey and Demeyer~\cite{rompaey09:_estab_traceab_links_unit_test} evaluate six other heuristics for finding focal classes: naming conventions, types referred to in tests (``fixture element types''), the static call graph, the last call before the assert, lexical analysis of the code and the test, and co-evolution of the test and the main code. No heuristic dominates: different heuristics work for different codebases. Our approach adds another way to rule out unwanted focal method and focal class results.

%% Ying and Tarr~\cite{DBLP:conf/eclipse/YingT07} also propose heuristics to filter out unwanted methods during code inspection. Their heuristics are based on characteristics of the call graph, i.e. they filter out small methods and methods closer to the bottom of a call graph, depending on tuneable parameters. These heuristics empirically eliminate mock calls in their benchmarks, but there is no principled reason for that to be the case, and indeed, the static call graph that they depend on should not interact well with mock calls.

%% \paragraph{Treatment of containers} In this work, we use coarse-grained abstractions for containers, consistent with the approach from Chu et al~\cite{chu12:_collec_disjoin_analy}. We do not observe sophisticated container manipulations where it would be necessary to track exactly which elements of a container are mocks. Were such an analysis necessary, the fine-grained container client analysis by Dillig et al~\cite{dillig11:_precis_reason_progr_using_contain} would work.

%% \paragraph{Taint analysis} Like many other static analyses, our mock analysis can be seen as a variant of a static taint analysis: sources are mock creation methods, while sinks are method invocations. There are no sanitizers in our case. However, for a taint analysis, there is usually a small set of sink methods, while in our case, every method invocation in a test method is a potential sink. In some ways, our analysis resembles an information flow analysis like that by Clark et al~\cite{clark07:_static_analy_quant_infor_flow}. However, the goal of our analysis (detecting possible mocks) is different from taint and information flow analyses in that it is not security-sensitive, so the balance between false positives and false negatives is different---it is less critical to not miss any potential important mock invocations, whereas missing a whole class of tainted methods would often be unacceptable.


%% \paragraph{Imperative vs declarative}
%% Kildall contributed perhaps the first dataflow analysis~\cite{kildall73:_unified_approac_global_progr_optim} as the concept is understood today, describing an algorithm for intraprocedural constant propagation and common subexpression elimination. His algorithm, operating on the program graph, is described in quite imperative pseudocode (and proven to terminate). In some sense, implementing algorithms imperatively is the default, and doesn't need further discussion, except to point out that program analysis frameworks such as Soot~\cite{Vallee-Rai:1999:SJB:781995.782008} provide libraries that can ease the implementation burden.

%% To our knowledge, Corsini et al did some of the first work in declarative program analysis~\cite{corsini93:_effic}; however, that work performed abstract interpretation on (tiny) logic programs rather than imperative programs. Dawson et al~\cite{dawson96:_pract_progr_analy_using_gener} did similar work. Around the same time, Reps proposed~\cite{Reps1995} a declarative analysis to perform demand versions of interprocedural program analyses, which is similar to what we have here; however, we compute all of the analysis results rather than performing a demand analysis. CodeQuest by Hajijev et al~\cite{hajiyev06} also allows developers to perform AST-level code queries using a declarative query language. {\sc Dimple$^+$}~\cite{benton07:_inter_scalab_declar_progr_analy}\cite[Chapter 3]{benton08:_fast_effec_progr_analy_objec_level_paral} by Benton and Fischer may be closest to what we are advocating as the declarative analysis approach. While Benton's dissertation presents a simple {\sc Dimple$^+$} implementation of Andersen's points-to analysis, the {\sc Dimple$^+$} work does not have Doop's sophisticated pointer analysis available to it. Soufflé, by Scholz et al~\cite{scholz16:_fast_large_scale_progr_analy_datal}, advocates for declarative static analysis (but without comparing it directly to an imperative approach as we do here), and presents performance optimizations needed to achieve this goal.
%% Finally, Doop~\cite{bravenboer09:_stric_declar_specif_sophis_point_analy}, which is now primarily implemented with a Soufflé backend, is perhaps the most powerful extant declarative program analysis, and focusses on expressing sophisticated pointer analyses in Datalog. 



%% % implementation note: Reps's approach is much more complicated than what we have in Doop. Perhaps Doop's use of SSA and simulation of phi nodes allows it to use much simpler rules, or maybe it's the specific analyses that are being implemented. e.g. for Doop, which computes an overapproximation, merging the two branches using the virtual phi node (simulated as "x = phi(x1,x2) => x = x1; x = x2") works just fine.

%% In terms of comparing implementations, Prakash et al~\cite{prakash21:_effec_progr_repres_point_analy} compare pointer analysis as provided by Doop and Wala; in some sense, the present work is similar to that work in that both works compare two frameworks. However, that work compares empirical results from two families of pointer analysis implementations (and finds that the specific intermediate representation used doesn't change the results much), while we discuss the process of implementing a static analysis declaratively versus imperatively. Like us, they note that Doop is difficult to incorporate into a program transformation framework (it works better in standalone mode) while Wala's results are readily available; a similar result applies to any result that a Soot-based data flow analysis produces as compared to a Doop-based declarative analysis.


